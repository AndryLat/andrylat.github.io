[{"categories":["AI Projects"],"contents":"Since 2019, the raging COVID-19 epidemic has claimed the lives of many people and forced many countries to take measures to blockade and isolate. Exercising at home is becoming a new way to improve people's health and protect them from the Corona virus. Many people responded to the 30-day push-ups challenge with the hashtag #pushupchallenge on social networks as a way to encourage each other to take some physical activities. My team has designed and built a desktop app that can analyze and count the number of pushups. With this intelligence feature and an attractive user interface, our application is expected to make fitness more enjoyable. This post will share with you our design and implementation of this idea.\nFirst, I want to share with you our interesting demonstration before going into details.\n  1. System Analysis and Design Sensor-based pushup counter The sensor-based implementations of a pushup counter require the user to touch or come close to a proximity sensor. You can find some patents, DIY projects or some apps using this approach below. This kind of implementation depends on some specific sensors, or hardware mechanisms to run. We think that using a touch button or proximity sensor for counting pushups is not a convenient way for the user. This approach is also hard to be extended to use with other workout activities.\nExamples of sensor-based method:\n Push up device: https://patents.google.com/patent/US8998783B2/en. Arduino Pushup Counter: https://create.arduino.cc/projecthub/mixpose/pushup-counter-e3ed69. Push Up Counter App - Google Play Store: https://play.google.com/store/apps/details?id=michaelbosch.com.pushupscounter\u0026amp;hl=en_US\u0026amp;gl=US.  Computer vision approaches Computer vision approaches are expected to be a more interesting and more general way to sensor-based methods. Using a camera, we can analyze different kinds of workout activities, and also deploy other ideas like workout pose correction. Let's look into some methods.\nDifferent approaches for pushup counting General counters: Google RepNet (link, paper) is a SOTA method of general counter, where we can feed a video stream in and receive the counting. This approach can be used to count multiple activities with the network. However, this kind of network works not very well when the period of the activity is unstable. This architecture also requires a huge amount of computation, which is not suitable for running in realtime on weak desktop PCs or mobile devices.\nOther time series methods on video: We did some experiments with deep neural architectures to analyze video stream (3D-CNN + Linear, CNN + LSTM) and trigger a count whenever a pushup is finished. However, these architectures also met performance issues and could not capture the activity period well.\nImage processing + Signal processing: This post talks about a naive method to analyze the positions of moving pixels using signal processing and count the peaks for pushups. However, we suppose that this method suffers from wrong counting when the user doesn't do pushup. The counting result is also affected by environmental factors such as other moving objects or changing light conditions.\nOptical flow: This repository brought us the idea of using optical flow. They use a neural network to classify dense optical flow frames into \u0026quot;Moving Up\u0026quot;, \u0026quot;Moving Down\u0026quot; or \u0026quot;Not Moving\u0026quot;. However, we suppose that the direction of the movement can be regressed only by using the main angle of the optical flow vectors, so it's unnecessary to use the neural network here. We also tried to analyze the average angle of the optical flow vector with signal processing and obtained an optimistic result. Still, we don't want to use this method because the magnitude of optical flow vectors depends a lot on the speed of movement and this may cause wrong counts.\nOur approach In our design, we combine keypoint detection and signal processing to count the pushups. In order to eliminate wrong counting when the users do other activities, we use a pushup recognition network to recognize pushup activity from the video stream. Using keypoint detection opens a new development idea: analyze pushup pose and give warnings when users do pushups in a wrong way.\nThe main flow of our application. The human pose image from Skeletal Graph Based Human Pose Estimation in Real-Time - M. Straka, Stefan Hauswiesner, H. Bischof\n2. Human keypoint detection Human keypoint detection (or human pose estimation) is defined as the problem of localization of human joints (also known as keypoints - elbows, wrists, etc) in images or videos.\nDatasets In this project, we only detect 4 keypoints: head, 2 shoulders, 2 hands, and 2 wrists. Our dataset was built up with 11503 images from MPII Human Pose Dataset and 11039 images from crawled Facebook videos. The distribution of our dataset is described below.\nHuman Keypoint Detection dataset\nMetric In order to measure the quality of the models, we define Percentage of Correct Keypoints shoulder (PCKs) metric. A detected joint is considered correct if the distance between the predicted and the true joint is within a certain threshold. The threshold here is chosen as 0.25 times of the distance between 2 wrists (or the distance between point 5 and point 6 in the image).\nPCKs metric\nModel architecture We propose a heatmap-based architecture to detect keypoints. In this architecture, we did some experiments with 3 backbones: ResNeSt50, ShuffleNet, and MobileNet). You can find our experimental code here.\nOur proposed architecture - Built from scratch\nBased on BlazePose, a lightweight convolutional neural network architecture for human pose estimation, we also design a lightweight heatmap-based architecture by using some building blocks of this network. Our implementation can be found here.\nOur proposed architecture based on heatmap branch of BlazePose model\nOur results:\n   Model PCKs MAE # of Params     ResNeSt backbone 0.818 0.028 10,563,989   ShuffleNet backbone 0.766 0.032 1,557,443   MobileNet backbone 0.786 0.029 1,910,437   BlazePose - Heatmap branch 0.760 0.032 885,559    3. Use signal processing to count Use signal processing to count Pushups\nIn the first implementation, we pick the head keypoint and design an algorithm to count pushups using the position of this point. The position of the head point is considered as a time-series signal. In order to count pushups, we apply 3 steps:\n Step 1: Filter the signal using a low-pass filter. Step 2: Calculate the adaptive mean and standard deviation of the signal over time. At time point $t$, the signal value is denoted as $s(t)$. The sequence $s(t-1), s(t-2), ..., s(t-n)$ has mean $\\mu_{s(t-1)}$ and standard deviation $\\sigma_{s(t-1)}$, with $n$ is the length of sampling window. Step 3: Calculate current frame label value:   $$ \\text { label }=\\left\\{\\begin{array}{cc} 1 \u0026 \\text { if } s(t)\\mu_{s(t-1)}+h * \\sigma_{s(t-1)} \\\\ 0 \u0026 \\text { otherwise } \\end{array}\\right. $$  1 is corresponding to the high position of head points, 0 is corresponding to other positions. $h$ is a configurable parameter.  We count the number of alters between two labels for the number of pushups.\n4. Pushup recognition The signal processing method can cause redundant counts when the user is not pushing up. We use another classification network based on MobileNetV2 to recognize when the user is pushing up to eliminate wrong counts. This network receives the image as input and answers the question \u0026quot;is the user pushing up?\u0026quot;. Although we haven't had much time to optimize this network, it worked.\nPushup recognition network based on MobileNetV2\n5. Desktop app implementation Our project is for educational purposes only so that we design a simple user interface with OpenCV. Below is the main screen of our application. Click the image to open Youtube video.\nThe demonstration of GUI\n\n6. Conclusion We had to finish our pushup counter in a limited time (~2 weeks). Therefore, we believe that it's certainly not the best result of our approach. It can be improved a lot more, for example, bettering counting result with more keypoints or leveraging the keypoints to correct wrong pushup poses. You can also use our code to train models and build the counter for other workout activities.\nWe published our application source code at https://github.com/VNOpenAI/pushup-counter-app.\nOther resources  Our team: https://vnopenai.org/our-team/. RepNet https://arxiv.org/pdf/2006.15418.pdf. Workout Type Recognition and Repetition Counting with CNNs from 3D Acceleration Sensed on the Chest https://ambientintelligence.aalto.fi/team_old/findling/pdfs/publications/Skawinski_19_WorkoutTypeRecognition.pdf. Recognition and Repetition Counting for Complex Physical Exercises with Deep Learning https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6387025/.  ","permalink":"https://andrylat.dev/posts/2021-02-15-build-a-pushup-counter/","tags":["Keypoint Detection","Signal Processing","Counter"],"title":"Build a Pushup counter app with OpenCV and Deep learning"},{"categories":["Self-Driving Cars"],"contents":"This is an introduction course to self-driving cars and Apollo platform - The Android of Self-Driving Car. Through this course, you will be able to identify key parts of self-driving cars and get to know Apollo architecture. You will be able to utilize Apollo HD Map, localization, perception, prediction, planning and control, and start the learning path of building a self-driving car.\nMy notes for this course:\n Lecture 1: SCD Fundamentals Lecture 2: HD Maps Lecture 3: Localization Lecture 4: Perception Lecture 5: Prediction Lecture 6: Planning Lecture 7: Control  ","permalink":"https://andrylat.dev/posts/apollo-sdc-lessons-intro/","tags":["Computer Vision","Apollo"],"title":"[MOOC] Apollo Lessons on Self-Driving Cars"},{"categories":["Self-Driving Cars"],"contents":"This is my note for lesson 3 of MOOC course: Self-Driving Fundamentals - Featuring Apollo. Content: How the vehicle localizes itself with a single-digit-centimeter-level accuracy.\nLocalization methods in Apollo  The RTK (Real Time Kinematic) based method which incorporates GPS and IMU (Inertial Measurement Unit) information. The multi-sensor fusion method which incorporates GPS, IMU, and LiDAR information.  Inertial navigation Global Navigation Satellite System (GNSS) Global Navigation Satellite System (GNSS) refers to a constellation of satellites providing signals from space that transmit positioning and timing data to GNSS receivers. The receivers then use this data to determine location. Global Positioning System (GPS) is a kind of GNSS.\nProperties:\n Accurate with RTK Poor performance in urban area and canyons Low frequency update (~10Hz) ➝ Too slow for realtime positioning on SDC.  Inertial Measurement Unit (IMU) On Wikipedia: An inertial measurement unit (IMU) is an electronic device that measures and reports a body's specific force, angular rate, and sometimes the orientation of the body, using a combination of accelerometers, gyroscopes, and sometimes magnetometers.\nComponents of IMU:\n Accelerometer: measures velocity and acceleration Gyroscope: measures rotation and rotational rate Magnetometer: establishes cardinal direction (directional heading)  Disadvantage: The IMU's motion error increase with time.\nGPS + IMU We can combine GPS + IMU to localize the car. On one hand, IMU compensates for the low update frequency of GPS. On the other hand, GPS corrects the IMU's motion errors.\nLiDAR Localization With LiDAR, we can localize a car by means of point cloud matching. This method continuously matches the detected data from LiDAR sensors with the preexisting HD map. ➝ Require constantly updated HD map ➝ Very difficult.\nVisual localization Can we use images from cameras to localize the car?\nYes, but using only camera is hard. We often combine images with other sensor signals.\nParticle Filter: We use particles or points on the map to estimate our most likely location.\nApollo Localization Apollo localization using input from multiple sources and use Kalman Filter for sensor fusion.\nSensor fusion for localization\nKidnapped Vehicle TODO: Try Kidnapped Vehicle Project.\n","permalink":"https://andrylat.dev/posts/apollo-sdc-lesson-3-localization/","tags":["Computer Vision","Apollo"],"title":"[MOOC] Apollo Lesson 3: Localization"},{"categories":["Self-Driving Cars"],"contents":"This is my note for lesson 2 of MOOC course: Self-Driving Fundamentals - Featuring Apollo. Content: High Definition maps for self driving cars.\nHD Maps have a high precision and contain a lot of information than your ordinary map on smartphone, such as lane line markings, 3D representation of the road network, traffic signs... You can what you see and GPS to locate your self in the world and identify other objects. However, it's very difficult with a self driving car, so we need HD Maps for current SDCs.\nHD Maps\nPrecision  Navigation map on your phone: meter-level precision HD Maps: centimeter-level precision  Localization on HD Maps Self-driving car uses sensor and camera signals to recognize where it is on HD Map.\nLocalization on HD Maps\nStandard  Apollo uses OpenDRIVE map format, and improve it to become Apollo OpenDRIVE standard.  Map construction  Steps for map production:  Map production\n","permalink":"https://andrylat.dev/posts/apollo-sdc-lesson-2-hd-maps/","tags":["Computer Vision","Apollo"],"title":"[MOOC] Apollo Lesson 2: HD Maps"},{"categories":["Self-Driving Cars"],"contents":"This is my note for lesson 1 of MOOC course: Self-Driving Fundamentals - Featuring Apollo. Content: Identify the key parts of self-driving cars. The Apollo team and architecture.\nHuman vs Self-driving Car    Human Self-Driving Car     High traffic accident rate More reliable driving   Learn to drive from scratch Learnable driving system   Parking trouble No parking trouble    Six levels of self-driving car   Level 0: Base level - No autonomous task\n  Level 1: Driver assistance\n Driver Fully Engaged    Level 2: Partial Automation\n Automatic Cruise Control Automatic Lane Keeping    Level 3: Conditional Automation\n Human Take Over Whenever Necessary    Level 4: No Human Interference\n Without Steering Wheel, Throttle or Brake Restricted in Geofence    Level 5: Full Automation\n  Apollo platform 1. Hardware The hardware system of a self-driving car. Image from Apollo course\n  The Controller Area Network (CAN) cars is how the computer system connects to the car internal network to send signals for acceleration, braking and steering.\n  The Global Positioning System (GPS) receives signals from satellites, circling the earth. These signal help to determine our location.\n  The Inertial Measurement Unit (IMU) measure the vehicle movement and location by tracking the position, speed, acceleration and other factors.\n  LiDAR is an array of pulse layers. The LiDAR of Apollo can scan 360 degrees around the vehicle. The reflection of these lazer beams builds the point cloud that our software can use to understand the environment.\n  Cameras can be used to capture environment. For example because cameras can perceive color, they can be use to detect and understanding traffic lights.\n  Radar is also used for detecting obstacle. However, it's difficult to understand what kind of obstacle that radar has detected. Advantages: it's economical, it works in all weather and lighting condition.\n  2. Open Software Stack Sublayers:\n Real-time operating system (RTOS) Runtime framework Application modules  Real-time operating system (RTOS) Apollo RTOS\nApollo RTOS is a combination of Ubuntu linux and the Apollo kernel.\n Ubuntu is popular but not a RTOS. Ubuntu + Apollo kernel -\u0026gt; RTOS.  Runtime framework: Customized ROS (Robot Operation System) To adapt ROS for self-driving cars, the Apollo teams has:\n Improve functionality Improve performance for shared memory, decentralization and data comparability  Apollo uses shared memory\nShared memory pattern\nApollo decentralize ROS architecture\n The original architecture of ROS:  ROS Master fails\n Decentralized architecture of ROS:  Decentralized Architecture\nApollo used Protobuf instead of native ROS Message for data comparability between different versions of the system\n3 Apollo Cloud Service  HD Map Simulation Data platform Security OTA  Apollo Github Link to Github repo: https://github.com/ApolloAuto/apollo.\n","permalink":"https://andrylat.dev/posts/apollo-sdc-lesson-1-fundamentals/","tags":["Computer Vision","Apollo"],"title":"[MOOC] Apollo Lesson 1: SDC Fundamentals"},{"categories":["AI Projects"],"contents":"Recently, I have built a prototype of an advanced driver-assistance system (ADAS) using a Jetson Nano computer. In this project, I have successfully deployed 3 deep neural networks and some computer vision algorithms on a super cheap hardware of Jetson Nano. In the last two posts, I have introduced the system in hardware and software design. In this week, I write about two machine learning modules: Object Detection Module and Lane Detection Module. I will focus on three core deep neural networks: an object detection network based on CenterNet, a ResNet-18 based traffic sign classification network and a U-Net based lane line segmentation network. I also introduce some experimental results in training and optimizing these networks.\nI. Object Detection Module Based on hardware constraints described in the first post of this series, in this section, I will introduce one of the key modules in machine learning block - Object Detection Module. This module is responsible for detect front obstacle objects such as other vehicles or pedestrians, and traffic signs. These results can be used for forward collision warning and over-speed warning. To provide these functions, the module contains two main components: a CenterNet based object detection neural network and a ResNet-18 based traffic sign classification network.\nObject detection module design\n1. CenterNet-based Object detection network Traffic object detection is a key deep neural network which contributes in forward collision warning and overspeed warning functions in my system.\nBackground Recently, a trend in object detection improvement is to treat object detection as key point estimation problem. CenterNet Objects as Points introduced in 2019 uses keypoint estimation to find object center points and regresses to all other object properties, such as size, 3D location, orientation, and even pose. The simplicity of this method allows CenterNet to run at a very high speed and outperform a range of state-of-the-art algorithms. In this object detection module, I choose CenterNet as the main object detection network because its simplicity and efficiency make it suitable for embedded hardware. I trained CenterNet using Berkeley DeepDrive (BDD) dataset with 10 classes: person, rider, car, bus, truck, bike, motor, traffic light, traffic sign and train. I have a blog post about CenterNet here (It's only available in Vietnamese).\nBackbones In this project, I use MobileNetV2 and ResNet-18 as the backbone of CenterNet.\nNowadays, state-of-the-art CNN architectures go deeper and deeper. While AlexNet had solely 5 convolutional layers, the VGG network and GoogleNet had 19 and 22 layers respectively. However, increasing network depth does not work by merely stacking layers along. Deep networks are hard to train because of the vanishing gradient problem - as the gradient is back-propagated to earlier layers, repeated multiplication may make the gradient become very small. As a result, once the network goes deeper, its performance gets saturated or begins to degrade quickly. The core idea of ResNet solution is introducing an “identity shortcut connection” that skips one or more layers. Following is the image of how to construct an “identity shortcut connection”. Instead of learning a direct mapping of $x \\rightarrow y$ with a function $H(x)$, let us define the residual function using $F(x) = H(x) - x$, which can be reframed into $H(x) = F(x) + x$, where $F(x)$ and $x$ represents the stacked non-linear layers and the identity function respectively. The author’s hypothesis is that it is easy to optimize the residual mapping function $F(x)$ than to optimize the original, unreferenced mapping $H(x)$. By this way, they can construct networks with much more layers. ResNet-18 is a lightweight 18-layer network with residual blocks.\nResidual Block - Image from ResNet paper\nMobileNetV2 is a lightweight architecture targeting in mobile and embedded devices. This network also uses residual architecture like ResNet. Moreover, depth-wise separable convolution is used which dramatically reduce the complexity cost and model size of the network.\nDataset BDD100K is used as the main dataset for object detection. In the original download website, authors divided their dataset into three subsets: training, validation, and test, which I will call original training set, original validation set, and original test set, respectively. I could not download the original test set because it was not available during my project schedule, so I only use original training and original validation sets of this dataset. BDD100K original validation set is setup up as test set in my experiments. Besides, I randomly split original training set into training set and validation set with ratio 85:15 for my experiments. Below is the distribution of the datasets used in this project.\nTable: Object detection test set: 10000 images (~12.5% number of images in the whole dataset)\n   Class person rider car bus truck bike motor traffic light traffic sign train     Number of images 13262 649 102506 1597 4245 1007 452 26885 34908 15    Table: Object detection training set: 59383 images (~74% number of images in the whole dataset)\n   Class person rider car bus truck bike motor traffic light traffic sign train     Number of images 77637 3853 605279 9950 25493 6157 2576 157491 203297 119    Table: Object detection validation set: 10480 images (~13.5% number of images in the whole dataset)\n   Class person rider car bus truck bike motor traffic light traffic sign train     Number of images 13712 664 107932 1722 25493 1053 426 28626 36389 17    All datasets are converted into COCO-like object detection format.\nExperiments ResNet-18 and MobileNetV2 are two lightweight backbones which are used in my experiments. For training, I use the source code from CenterNet’s authors with some modifications:\n ResNet-18 backbone was implemented in the original source code from CenterNet’s authors. This source code was written with PyTorch framework. I trained this network with three image sizes 224x224, 384x384, 512x512 and batch size 32. Learning rate is set to $10^{-4}$, reduced to $10^{-5}$ on epoch 90, $10^{-6}$ on epoch 120. These setups were trained in 140 epochs. Besides ResNet-18, MobileNetV2 backbone was added into the original source code. I trained this network with three image sizes 224x224, 384x384, 512x512 and batch size 32. Learning rate is set to $5x10^{-4}$, reduced to $5x10^{-5}$ on epoch 35. These setups were trained in 70 epochs.  Result\nThe mean average precision and the inference speed of trained models are described in below table (system configuration: Intel Core i5 8400 and NVIDIA RTX 2070). As shown in the table, the inference time of CenterNet – MobileNet V2 models are lower than CenterNet – ResNet-18 models. However, these models are also less accurate than CenterNet – ResNet-18 models.\nTable: Mean average accuracy and inference time of CenterNet models. mAP: mean average precision, IoU: intersection over union.\n\nModel optimization for embedded hardware PyTorch to TensorRT engine conversion\nAfter training CenterNet using PyTorch framework, we obtain model files in PyTorch model format (.pth). In order to optimize inference speed on NVIDIA Jetson Nano, we need to convert these models to TensorRT engine file. The conversion is done via an intermediate format called ONNX (Open Neural Network Exchange). PyTorch model is converted to ONNX format first using PyTorch ONNX module (step 1). After that, we convert ONNX model to TensorRT engine for each inference platform (step 2). Because the conversion from ONNX to TensorRT engine takes a long time, in my implementation, I serialize TensorRT engine to hard disk after converting and load it every time the program starts. In this step, we have to notice that TensorRT engine is built differently on different computer hardware. Therefore, we need to rebuild the engine if we need to inference on other hardware configuration.\nAfter the engine conversion, I test the result on the test set. The accuracy and inference time of the models are described in following table. The inference time of CenterNet – MobileNet V2 models are lower than CenterNet – ResNet-18 models. However, these models are also less accurate than CenterNet – ResNet-18 models. The accuracy difference between the original models and the converted models is negligible. The difference in accuracy between float 16 and float 32 precision is also small (mAP columns). However, the efficiency of the conversion process can be clearly seen (comparing the inference time of models for the original PyTorch framework and the converted models).\nTable: Accuracy and inference time comparison after TensorRT engine conversion – Object detection model. The evaluation is done on the test set\n\nBased on the evaluation result, we can see that CenterNet with two lightweight backbones ResNet-18 and MobileNetV2 can achieve an acceptable accuracy and relatively high inference speed. The speed of these networks can be pushed further by running in float 16 precision without a noticeable reduction in accuracy. ResNet-18 backbone can be optimized better than MobileNetV2 in term of speed, which can be recognized by a larger reduction in inference time after converting. As CenterNet model with ResNet-18 backbone and input image size 384x384 provides the best balance between speed and accuracy on Jetson Nano, it is chosen as the main architecture for this project.\n2. Traffic sign classification network Due to the limitation of BDD dataset - It's only contains 1 class for traffic signs (without specifying the sign type), I had to train another neural network to recognize sign types. Because of the high speed and accuracy, ResNet-18 was also chosen for this task. I trained the model using Tensorflow and Keras frameworks. However, to optimize the speed, I also convert final model to TensorRT format.\nDataset In this project, I only design the system to classify maximum speed signs, and I treat each speed level as a separate object class. In order to collect enough data for training, I use 2 datasets: Mapillary Traffic Sign Dataset (MTSD) and German Traffic Sign Recognition (GRSRB) dataset. As MTSD is a traffic sign detection dataset, I use sign bounding boxes to crop them for classification task. After cropping, I merge 2 datasets and have 18,581 images of maximum speed limit traffic signs divided into 13 classes, and 879 end of speed limit signs (treating all end of speed limit signs as only 1 class). Besides, I use 20,000 crop images from other traffic signs and objects for “unknown” class. In total, there are 15 classes in this dataset: maximum speed signs (5km/h, 10km/h, 20km/h, 30km/h, 40km/h, 50km/h, 60km/h, 70km/h, 80km/h, 90km/h, 100km/h, 110km/h, 120km/h), end of speed limit (EOSL) and other sign (OTHER). After that, this dataset is divided into 3 subsets: training set (80%), validation set (10%) and test set (10%). The distribution is randomly for each traffic sign class. The final result is described in following tables.\nTable: Number of images for each traffic sign class in training set. SL_X: maximum speed limit X km/h. EOSL: End of speed limit. OTHER: other objects – unknown class\n   SL5 SL10 SL20 SL30 SL40 SL50 SL60 SL70 SL80 SL90 SL100 SL110 SL120 EOSL OTHER     192 377 632 2312 1328 2164 1677 1677 1662 517 1105 251 989 700 16000    Table: Number of images for each traffic sign class in validation set. SL_X: maximum speed limit X km/h. EOSL: End of speed limit. OTHER: other objects – unknown class\n   SL5 SL10 SL20 SL30 SL40 SL50 SL60 SL70 SL80 SL90 SL100 SL110 SL120 EOSL OTHER     23 46 79 288 166 270 209 209 207 64 138 31 123 87 2000    Table: Number of images for each traffic sign class in test set. SL_X: maximum speed limit X km/h. EOSL: End of speed limit. OTHER: other objects – unknown class\n   SL5 SL10 SL20 SL30 SL40 SL50 SL60 SL70 SL80 SL90 SL100 SL110 SL120 EOSL OTHER     23 46 79 288 166 270 209 209 207 64 138 31 123 87 2000    Experiments I trained traffic sign classification network with image size 64x64, Adam optimizer with learning rate 0.0001, batch size 32, loss function categorical cross entropy. In order to deal with class imbalance problem, I set loss weight for class OTHER to 0.001 and for other classes to 1.0. This setup was trained in 45 epochs and I consider the lowest validation loss model at epoch 39 as the best model. My best model achieves micro F1 = 0.984 on the test set.\nModel optimization for embedded hardware The optimization from trained Keras model to TensorRT engine goes through 3 steps: freeze model from Keras model format to Tensorflow frozen graph, convert from frozen model to UFF (Universal Framework Format) and finally to TensorRT engine. UFF is recommended by NVIDIA as an immediate model format to convert deep learning models for TensorRT framework. The conversion is demonstrated in following figure.\nTraffic sign detection model to TensorRT engine conversion\nAfter testing TensorRT models, float 32 and float 16 precision model achieves F1 metric equal to 0.9843 and 0.9840 respectively on the test set. There is a small difference between these models and original Keras model (F1 = 0.9843). The speed is also improved in comparison with the original Keras model. The detail results are described in following table.\nTable: Accuracy and inference time comparison after TensorRT engine conversion – Traffic sign classification model. The evaluation was done on the test set\n\nDiscussion\nThe final traffic sign classification model has a high accuracy (mAP ~0.98), which is enough for production. This network is also fast: it can run at 1.9 ms per frame NVIDIA RTX 2070 and 6.6 ms per frame on Jetson Nano. TensorRT float 16 model was integrated in our system.\nII. Lane Detection Module Lane line detection module takes responsibility to detect lane lines and lane departure situation. This result is then used for lane departure warning. In this section, I describe the solution for lane line detection using deep neural network and Hough transform line detector.\nTraditional lane detection algorithms rely on highly specialized, handcrafted features to segment lane lines and fit lines using Hough transform or B-Snake-based algorithm. Popular representatives of this algorithm type use color-based features in combination with Hough transform, Canny edge detection algorithm in combination with Hough transform, ridgeness feature and RANSAC algorithm. In general, these traditional approaches are prone to robustness issues due to road scene variations (time of the day, weather condition, faded lane lines), and only performs well in some specific situations. Currently, the evolution of deep learning algorithms provides us better tools to build robust lane detection algorithms. U-Net is a fully convolutional network that works well in biomedical image segmentation, it can demonstrate high-precision segmentation results with less training image data. I applied U-Net to lane line segmentation problem and combined with Hough transform to find lane lines in form of line equations.\nThe main principle of this module can be described in following figure. From RGB image captured using car dash cam, the system uses a U-Net based lane line segmentation neural network to output a binary image, in which, pixels corresponding to lane lines is in white. From this binary image, the system uses Hough line transform to detect lines in the image and group them into lane lines using spatial information. After that, we can find left and right lane lines in the third step and determine lane departure situation.\nLane detection flow\n1. Lane line segmentation network In order to have a light-weight segmentation model to run on embedded hardware, I had two modifications with the original U-Net model: (I) adjust number of decoder filters to 128, 64, 32, 16, 8 filters from the top of decoder to the output of the network; (ii) replace the original backbone with ResNet-18 backbone. These modifications reduce the number of parameters in U-Net and give us a light-weight model which can run at over 200 frame per seconds (FPS) (model U-Net ResNet-18 input size 384x382, TensorRT float 16 on RTX 2070 GPU).\nDataset Dataset is prepared from Mapillary Vista dataset with some modifications. The original dataset contains about 18000 images in training set and about 2000 images in validation set. I merge these sets, remove a number of images which do not contain lane line or have too much noise. The final dataset has 15303 images. I split this set randomly into three subsets: 10712 images for training (~70%), 2296 images for validation (~15%) and 2295 images for test (~15%). Because the label of Mapillary Vista contains many object classes, I keep only lane line class to generate binary segmentation masks as the new label.\nMapillary Vistas dataset preprocessing – Images A, B are from Mapillary Vitas\nExperiments I did some experiments on prepared dataset with different training configurations. I depend on mean IoU (intersect over union) as the metric to optimize this network. All models were trained with stochastic gradient descent (SGD) optimizer with learning rate 0.001, momentum 0.9.\nTable: Mean IoU with different image sizes and loss functions. The evaluation is done on the validation set.\n\nAbove table describes the result. Because lane line segmentation suffers from data imbalance (the number of pixels belonging to lane lines is relatively small than the number of other pixels), the combination of Focal loss and Dice loss or the combination of Focal loss and Tversky loss produce better results than traditional binary cross entropy loss. This network can run at 11-13 ms per frame on the experimental system with Intel core i5 and NVIDIA RTX 2070 GPU.\nFinally, we tested the trained models on the test set. The best models we obtained were trained with the combination of Focal loss and Dice loss. These models can achieve mean IoU 0.736 with input image size 384x384 and mean IoU 0.744 with input image size 448x448.\nModel optimization for embedded hardware We convert lane line segmentation model in the same way as traffic sign classification model. After the conversion, following table shows the results of different configurations. I ran the experiments with 2 image size – 384x384 and 448x448. There is only a little difference in mean IoU metric between the models before and after conversion.\nMean IoU and inference time comparison after TensorRT engine conversion – Lane line segmentation model. Evaluation was done on the test set.\nDiscussion The trained models can achieve an acceptable mean IoU on the test set (0.736-0.745), which are suitable for production usage. We also have a high frame rate (384x384 – TensorRT float 16 model can run at over 200 FPS on RTX 2070 and 14 FPS on Jetson Nano). Finally, I integrated 384x384 – TensorRT float 16 model in the final system.\n2. Lane line detection with Hough transform Hough Transform is a line detection algorithm that is quite effective in image processing. The general idea of this algorithm is to create mapping from image space (A) to a new space (B) that each line in space (A) corresponds to a point in space (B), each point in space (A) corresponds to a sinusoid in space (B). After projecting all points in (A) into sinusoids in space (B), we find the places with the highest intersection point density. These places then are projected to (A) into lines. By this way, Hough line transform algorithm can find lines in image space (A).\nThe progress to find lane line candidates is shown in following figure. From segmentation mask produced by line segmentation network, lane line detection module uses probabilistic Hough transform to detect raw line segments (1). After that, these lines are partitioned into groups using disjoint-set/union-find forest algorithm inverted by Bernard A. Galler and Michael J. Fischer in 1964. We use spatial distance and angle difference between lines to group line segments which belong to a same line. After step (2), we can see that different line groups are drawn in different colors. Step (3) receives these line groups as the input and fit a line through each group using maximum-likelihood estimation with L2 distance.\nLine candidate detection\nCombining lane segmentation model with above lane detection algorithms, this system can detect lane lines in different environments and determine lane departure situation. It creates a reliable input for lane departure warning module.\nSystem testing and Conclusion In this project, I implemented and tested the system in different situations with a simulation. All testing cases were made with videos from Berkeley DeepDrive dataset, CARLA simulator and self-recorded videos.\nFinally, this post summaried my results in machine learning module of my advanced driver-assistance system. Currently, only the inference source code for Jetson Nano was made public. In the future, I will make public other repositories for training and optimizing neural networks. Stay tuned! All comments are welcome. Thank you!\nUpdate 15/11/2020: I added some links to the source code of this project in the first post.\n","permalink":"https://andrylat.dev/posts/adas-jetson-nano-deep-neural-networks/","tags":["ADAS","Jetson Nano"],"title":"Advanced driver-assistance system on Jetson Nano Part 3 - Deep neural networks"},{"categories":["AI Projects"],"contents":"Recently, I have built a prototype of an advanced driver-assistance system (ADAS) using a Jetson Nano computer. In this project, I have successfully deployed 3 deep neural networks and some computer vision algorithms on a super cheap hardware of Jetson Nano. I decided to write this post series to share about how this system was designed and implemented. The first post is an introduction and the hardware design of my system. Today post will talk about the software design.\nThe software system provides three main functions: (i) forward collision warning with forward vehicles and pedestrians, (ii) lane analysis and lane departure warning, (iii) sign detection for maximum speed limit signs and over-speed warning. These functions will be described in next 3 sections.\nFigure 1. Three main functions of this system\nI. Forward collision warning Figure 2. Forware collision warning demonatration\nFigure 3. Forware collision warning mechanism\nAbove figure (Figure 3) shows the mechanism of forward collision warning. After obstacle detection step, the system has the bounding boxes of obstacles (the green boxes in Figure 2), including forward vehicles and pedestrians. Warning is only issued when the car speed is high enough. The collision warning is done by identifying the danger zone in front of the vehicle and determining whether the bounding box of the any object meets the danger zone. In Figure 2, the determined danger zone is the red area in front of the camera.\nDanger zone identification In order to identify danger zone in image space, the system has to determine the danger zone in meters on the road, and then convert it to a corresponding area on the image. In this system, I use a bird view image as an immediate between the camera image and the real-world distances (Figure 4). After calculating the danger zone in meters in real-world space, the system calculates a corresponding area on the bird-view image using a meter-to-pixel ratio. This area is then transformed to a danger zone on the camera image using a perspective transform matrix. The meter-to-pixel ratio and the perspective transform matrix should be setup in camera calibration process.\nFigure 4. Danger zone identification\nCamera calibration\nTransformation parameters include the meter-to-pixel mapping from the real-world distances to the bird view image space and the perspective transform matrices between the bird view image to the camera image. In order to calculate these parameters, I use following solution: put a red carpet in the front of the car, measure distances $W1$, $W2$, $L1$, $L2$ as shown in Figure 5. The images in Figure 5 were created using Unity framework.\nFigure 5. Measure distances to calculate image transformation parameters\nBy selecting 4 points on the image obtained from dash camera, and establishing corresponding points in the bird view reference image (Figure 6 - below), the system calculates perspective transform matrix ($H$) and inverse perspective transform matrix ($H’$) between the image space and the bird view space. The calculation of the correlation ratio between $L2$, $W2$ and corresponding distances in bird view space helps determine that each pixel in bird view image corresponds to how many meters in the real world. After this calibration, we save all measured parameters as new settings for camera.\nFigure 6. Picking 4 points on camera images and establishing 4 corresponding points on bird view image\nDangerous zone estimation\nFigure 7. Car front danger zone\nThis system depends on the car width $W1$ and the danger distance to estimate the danger zone in front of vehicle. We assume that $S$ is the distance that the vehicle can move to in the next $t = 1.5$ seconds. This distance is calculated using car speed $v$ (m/s) by the following equation:\n$$ S = v.t $$\nThe area of the danger zone is $S \\times W1$.\nAfter calculating this danger zone, the system establishes a danger zone in corresponding bird view reference image and convert this area to camera image space using perspective transform matrix ($H$).\nEstimate distances to objects ahead\nFigure 8. Calculate distance to a front objects\nAlthough our system determines the dangerous situation by finding the intersection between object bounding boxes and danger zone in the image space, it also estimates the distance to forward vehicle as an additional helpful information for driver. This estimation process is described in Figure 8. Assume we have an object bounding box from object detection module. We transform this box to bird view space using inverse perspective transform matrix $H’$. After finding the bottom point of this bounding box, we use the pixel-to-meters ratio calculated from camera calibration to find the distance to object in meters. This distance is then shown to driver as a useful information.\nII. Traffic sign detection and over-speed warning Figure 9 shows the traffic sign detection flow. First, the system needs to detect traffic signs from dash camera image. When a new traffic sign comes in, if it is an end-of-speed-warning sign, the system removes all over-speed warnings, otherwise, if it is a maximum speed limit sign, it shows this sign to the screen and remember the maximum speed level in 30 minutes. The traffic sign type is also read aloud by the system through the speakers. At the same time, the system continuously checks for over-speed situation and issue warning if needed (Figure 10). This process constantly gets the car speed in real time and compare received speed with the maximum speed limit and warns drivers when they go over speed.\nFigure 9 Traffic sign detection flow\nFigure 10 Over-speed warning flow\nIII. Lane departure warning Figure 11 describes the flow of lane departure warning. Lane analysis is only activated when car speed is high enough (higher than 30km/h) and turn signal is off. When these two conditions are satisfied, the system detect lane lines from dash camera image and analyze these lane lines. If the system recognizes lane departure situation, it issues a lane departure warning immediately by showing an alert to screen and playing alert sound through speakers.\nFigure 11. Lane departure warning flow\nLane line detection can be done using a semantic segmentation neural network and computer vision algorithm. The result of this process if lane line candidates. From lane line candidates found in lane line detection, we extend lines and find intersection points of these lines with left, right and bottom borders of the image (Figure 12). We consider the line corresponding to the left-most intersection point on the right as the right lane line and the line corresponding to the right-most intersection point on the left as the left lane line. The intersection points of right and left lane line are compared with the center point of bottom edge of the image to determine the lane departure situation.\nFigure 12. Find left and right lane lines\nAssume the intersection of the left lane line and the bottom edge of the image is left point A, the intersection of the right lane line and the bottom edge of the image is right point B, and the middle point of the bottom edge is C – center point (Figure 13). The distances from A, B to C is normalized into range from 0 to 1. A lane departure situation is determined when one of these conditions happens: (1) AC \u0026lt; d1 and BC \u0026gt; d2 or (2) AC \u0026gt; d2 and BC \u0026lt; d1. In this project, I choose d1 = 0.3 and d2 = 0.5.\nFigure 13. Normalized intersection points of lane lines and bottom edge\nIII. Software implementation Software packages There are six main modules in this system, which are: object detection module, lane detection module, sensing module, warning control module, user interface module and simulation. Figure 14 shows the design of software modules and the interactions with hardware part. In a production system like MobileEye 630, the system takes images from camera and car states (such as speed, turn signal) through CAN bus. However, in development stage of this project, I use Simulation module to simulate data from camera and CAN bus.\nFigure 14. Software modules and interaction with hardware. All software modules are in orange. Hardware components are in blue\nImage data from camera or simulation is passed through a machine learning block. There are two main modules here: lane detection module and object detection module, which are backed by machine learning algorithms. Lane detection module takes responsibility for lane line and lane departure detection, while object detection module detects forward vehicles, pedestrians, and traffic signs. In these modules, deep neural networks and computer vision algorithms are used to analyze images from dash camera. In order to give correct warnings, the system has to use other data from car such as speed and turn signal.\nAfter processing all image and sensor data, if the system recognizes a dangerous situation, user interface and warning control module are responsible for issue warnings. These two modules are wrapped in user interaction block and output results directly to a touch screen and speakers.\nUser interface The user interface of this project is designed and optimized for touch screen. Following are some main screens of the system.\nFigure 15. UI - Home screen\nFigure 16. UI - Camera calibration steps\nFigure 17. UI - Instruction screen\nFigure 18. UI - Distance measurement screen\nFigure 19. UI - Select-4-point screen\nDemonstration Figure 20. The final prototype\nFigure 21. Home screen\nFigure 23. Sign detection\nFigure 24. Lane departure warning\nTo sum up, this post talks about the software design of my advanced-driver assistance system on Jetson Nano. The next posts will be about the implementation of deep learning models, the conversion process to TensorRT engine, and how to optimize the system to run smoothly on Jetson Nano. If you want the video demonstation of the system, please visit the first post of this series. All comments are welcome. Thank you!\n","permalink":"https://andrylat.dev/posts/adas-jetson-nano-software/","tags":["ADAS","Jetson Nano"],"title":"Advanced driver-assistance system on Jetson Nano Part 2 - Software design"},{"categories":["AI Projects"],"contents":"Recently, I have built a prototype of an advanced driver-assistance system (ADAS) using a Jetson Nano computer. In this project, I have successfully deployed 3 deep neural networks and some computer vision algorithms on a super cheap hardware of Jetson Nano. I decided to write this post series to share about how this system was designed and implemented. In this series, I will introduce the overall design of the system, 3 deep neural networks I used for environment analysis and some tutorials on TensorRT - the core technology to optimize neural networks for NVIDIA's system. In this post, let's get started with an introduction to my project and the hardware design of this system.\nI. Introduction 1. Background and motivations Currently, smart driver assistance functions are gradually being improved and become a new criterion in the technology race among car manufacturers. However, there are a large number of old cars and also a large number of new low-end car models without an advanced driver-assistance system (ADAS).\nFor this market, technology companies also develop separated products to setup on used car models or car models without integrated ADAS. In this type of product, MobileEye 630 is a popular device, which is developed by MobileEye, a subsidiary of Intel. MobileEye 630 provides intelligent features such as forward collision warning (FCW), lane departure warning (LDW), intelligent high beam control (IHC), speed limit indication (SLI), and traffic sign recognition (TSR). In Vietnam, WebVision is a company specializing in providing dashcam products with intelligent driver assistance technologies. WebVision A69 AI with camera recording function, lane departure warning, forward collision warning, and moving reminder when the traffic light turns green. WebVision S8, in addition to the dashcam function, also warns drivers when they go over speed.\nIt cannot be denied that Intel Mobile Eye or WebVision systems have reached a relatively good level of perfection. However, the Mobile Eye 630 system, which is currently sold in Vietnam, lacks a user interface for drivers to calibrate the device easily. WebVision devices, although equipped with useful additional functions such as map navigation, however, ADAS features are only partially equipped. For example, the WebVision A69 AI does not have a sign recognition function, and the WebVision S8 product lacks two important functions of an ADAS system: collision warning and lane departure warning. The traffic sign recognition feature in WebVision S8 is also done using the map data stored in the device in combination with GPS instead of a camera, which requires frequent updates. This may not be feasible in practice, and it is not helpful when drivers drive to new areas. Through this analysis, I recognize the need for a better and more completed advanced driver-assistance system for old and low-end cars and it is my reason to develop this system.\n2. Purpose and scope The purpose of this project is to design a prototype of a completed advanced driver-assistance system targeting old and low-end cars that are not equipped with, or lack of some driver assistance functions. The implemented product should be a system with hardware and software to provide three main functions: (i) forward collision warning with forward vehicles and pedestrians, (ii) lane analysis and lane departure warning, (iii) sign detection for maximum speed limit signs and over-speed warning. In the scope of this project, because of the limitation in experimental conditions, I developed and used a simulation to provide camera stream and car data stream instead of using a real camera and a real connector to connect with car electronic system. However, the completed design with a physical camera and car connector is still considered.\n3 main functions of this system\n3. Solutions For the hardware of the proposed system, I choose NVIDIA Jetson Nano, a small, powerful computer that lets you run multiple neural networks in parallel to deploy the final system. Jetson Nano is suitable for this project as it is a powerful hardware architecture with a cheap price to deploy deep learning models. This will keep the production cost relatively low compared to other similar systems. I also attach a 5-inch screen and two small speakers to build a user interface.\nIn software design, I use three neural networks to build the core of the system. For collision warning, this system uses CenterNet - an object detection network with ResNet-18 backbone to achieve a good detection speed and acceptable accuracy. Besides, perspective transform with calibration is used to estimate the distances from system vehicle to other vehicles ahead. The results from the object detection network are utilized to detect the location of traffic signs. After that, the system crops all traffic sign images and passes them through a classification network employing ResNet-18 architecture to distinguish signs. For lane departure warning function, the combination of U-Net and ResNet-18 backbone is used for lane line segmentation and Hough line transform is utilized to find lane lines. After that, the detected lines are used to identify lane departure situation using a rule-based algorithm. After training and fine-tuning, three networks are optimized to run on embedded system of Jetson Nano computer using NVIDIA Tensor-RT technology. This technology from NVIDIA helps neural networks run faster with much lower memory consumption.\nII. Hardware design and implementation The Hardware Design\nHardware always plays an important role in any embedded system. It specified resource constraints that software has to be optimized on. The center component to process all inputs of this project is a Center processing computer. This computer receives two inputs: (i) images from a camera, and (ii) car sensor data such as car speed and turn signal. It takes responsibility to process these inputs to issue warnings when needed. In the scope of this project, due to the limited experimental condition, I implemented a simulation module to provide alternatives to the camera and the sensor reader inputs. In order to output warnings, the center processing computer is connected with a touch screen and speakers.\nBelow is the list of components used in this project. These components are chosen in consideration of hardware ability, size, and price. The case for the whole system is designed and finished using crystal plastic and laser-cutting technology.\n Jetson Nano Developer Kit https://developer.nvidia.com/embedded/jetson-nano-developer-kit. Sandisk Ultra 64GB class-10 SD card https://www.memoryzone.com.vn/the-nho-microsdxc-sandisk-ultra-64gb-80mbs-533x-2017. Wareshare 5-inch LCD touch screen https://www.waveshare.com/5inch-hdmi-lcd-h.htm. Wareshare 8Ω 5W Speaker https://www.waveshare.com/8ohm-5w-speaker.htm. 2-inch 5V cooling fan for Jetson Nano Acrylic clear case.  Jetson Nano computer\nReleased in March 2019 by NVIDIA, Jetson Nano is a powerful platform for deploying machine learning algorithms. Because of a small size board with a quiet strong GPU, it is suitable to be used as the center processing computer. One special feature of this computer in comparison with ones from other companies is that it can use TensorRT, an SDK (software development kit) for high-performance deep learning inference. This SDK includes a deep learning inference optimizer and runtime for low latency and high-throughput experience. In this project, this feature can be leveraged to run deep learning networks to analyze images from dash camera. A Sandisk Ultra 64GB class-10 SD card is used as the main disk memory. The detail of system configuration of Jetson Nano is listed below:\n CPU: Quad-core ARM Cortex-A57. GPU: 128-core NVIDIA Maxwell architecture-based. RAM: 4 GB 64-bit LPDDR4; 25.6 gigabytes/second.  This blog has a post on how to configure and optimize Jetson Nano for AI. You can get started with this post (only available in Vietnamese now).\nDisplay screen and speakers\nFor user interaction, Wareshare 5-inch touch screen (H model) is a good option for this project. It provides a large enough space for a comfortable user experience. Because Jetson Nano does not contain any sound card, the sound card from H model screen is a convenient way to deploy speakers. I use 5Ω - 8W dual speaker from Wareshare to play warning and notification sounds in designed system.\nSensor reader module\nIn order to determine car speed, we can use an (i) an indirect solution – estimate speed using GPS signal or (ii) a direct solution – read speed directly from the car electronic system. The first way, estimating speed from GPS can be easier and safer because we do not have to connect to the car sensing networks, which can result in incorrect interactions with car components. However, because this solution has a delay in speed estimation, we should not use it for a safety warning system. Nowadays, almost car is equipped with a Controller Area Network (CAN) as one of the main networks to exchange data between electronic components, providing us a standard way to read car sensors such as car speed or turn signal. This method is also integrated into my system.\nConnection between ECUs using CAN bus\nTo communicate with CAN bus of a car, we need a component called CAN bus reader, which is a USB-to-CAN adapter. However, in this project, due to the limitation in experimental condition, I only implemented a virtual CAN instead to exchange data between driving simulation and the core system.\nHow to connect hardware components?\nConnections between hardware components\nAt last, I want to show you a demonstration of my prototype for the system. Currently, my system can only run on video and simulated sensor streams. In the upcomming posts, I will talk more about the neural networks and the implementation of the software stack of this project.\n  III. Source code 1. Object detection with CenterNet  Training code for BDD100k dataset: https://github.com/vietanhdev/centernet-bdd-data. Conversion code to ONNX model: https://github.com/vietanhdev/centernet-bdd-data-onnx-conversion.  2. Lane line segmentation with U-Net  Training and conversion code to .uff: https://github.com/vietanhdev/unet-uff-tensorrt.  3. Traffic sign  Training and conversion code to .uff: https://github.com/vietanhdev/traffic-sign-classification-uff-tensorrt.  4. Code for Jetson Nano  Code for Jetson Nano - contains all inference code for above models: https://github.com/vietanhdev/car-smart-cam  Give me Github Star if you think it is interesting. Note that this repository does not contain the source code for training and converting AI models. I'll make them public as soon as possible.\nUpdate 12/10/2020: The next post is about the software of this project. Go to the next post now.\nUpdate 15/11/2020: Add links to source code.\n","permalink":"https://andrylat.dev/posts/adas-jetson-nano-intro-and-hardware/","tags":["ADAS","Jetson Nano"],"title":"Advanced driver-assistance system on Jetson Nano Part 1 - Intro \u0026 Hardware design"}]